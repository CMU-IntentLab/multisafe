<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Safe control techniques, such as Hamilton-Jacobi reachability, provide principled methods for synthesizing safety-preserving robot policies but typically assume hand-designed state spaces and full observability. Recent work has relaxed these assumptions via latent-space safe control, where state representations and dynamics are learned jointly through world models that reconstruct future high-dimensional observations (e.g., RGB images) from current observations and actions. This enables safety constraints that are difficult to specify analytically (e.g., spilling) to be framed as classification problems in latent space, allowing controllers to operate directly from raw observations. However, these methods assume that safety-critical features are observable in the learned latent state. We ask: when are latent state spaces sufficient for safe control? To study this, we examine temperature-based failures, comparable to overheating in cooking or manufacturing tasks, and find that RGB-only observations can produce myopic safety behaviors, e.g., avoiding seeing failure states rather than preventing failure itself. To predict such behaviors, we introduce a mutual information-based measure that identifies when observations fail to capture safety-relevant features. Finally, we propose a multimodal-supervised training strategy that shapes the latent state with additional sensory inputs during training, but requires no extra modalities at deployment, and validate our approach in simulation and on hardware with a Franka Research 3 manipulator preventing a pot of wax from overheating.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="latent-space safe control, partially observable safety constraints, mutual information, multimodal machine learning, robotics">
  <!-- TODO: List all authors -->
  <meta name="author" content="Matthew Kim, Kensuke Nakamura, Andrea Bajcsy">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Intent Robotics Lab">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Safe control techniques, such as Hamilton-Jacobi reachability, provide principled methods for synthesizing safety-preserving robot policies but typically assume hand-designed state spaces and full observability. Recent work has relaxed these assumptions via latent-space safe control, where state representations and dynamics are learned jointly through world models that reconstruct future high-dimensional observations (e.g., RGB images) from current observations and actions. This enables safety constraints that are difficult to specify analytically (e.g., spilling) to be framed as classification problems in latent space, allowing controllers to operate directly from raw observations. However, these methods assume that safety-critical features are observable in the learned latent state. We ask: when are latent state spaces sufficient for safe control? To study this, we examine temperature-based failures, comparable to overheating in cooking or manufacturing tasks, and find that RGB-only observations can produce myopic safety behaviors, e.g., avoiding seeing failure states rather than preventing failure itself. To predict such behaviors, we introduce a mutual information-based measure that identifies when observations fail to capture safety-relevant features. Finally, we propose a multimodal-supervised training strategy that shapes the latent state with additional sensory inputs during training, but requires no extra modalities at deployment, and validate our approach in simulation and on hardware with a Franka Research 3 manipulator preventing a pot of wax from overheating.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://cmu-intentlab.github.io/multisafe/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="static/images/unicycletoast.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/intent_logo_icon_transparent-highQuality (1).png">
  <link rel="apple-touch-icon" href="static/images/intent_logo_icon_transparent-highQuality (1).png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-2 publication-title">What You Don't Know <i>Can</i> Hurt You: <br> How Well do Latent Safety
              Filters Understand Partially Observable Safety Constraints?</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/mattkiim/" target="_blank">Matthew Kim</a><sup>*1</sup>,</span>
                <span class="author-block">
                  <a href="https://kensukenk.github.io/" target="_blank">Kensuke Nakamura</a><sup>*2</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.cmu.edu/~abajcsy/" target="_blank">Andrea Bajcsy
                    </a><sup>2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!--<span class="author-block">Institution Name<br>Conference name and year</span>-->
                    <span class="eql-cntrb" style="color: #333; font-size: 0.95rem; font-weight: 500;">
                      <br><sup>*</sup>Indicates Equal Contribution &nbsp;&nbsp;
                      <span style="color: #111;"><sup>1</sup>UC San Diego</span> &nbsp;&nbsp;
                      <span style="color: #111;"><sup>2</sup>Carnegie Mellon University</span>
                    </span>
                  </div>

          

                  <span class="link-block">
                    <a href="https://github.com/CMU-IntentLab/multisafe" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code [coming soon]</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv [coming soon]</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- simulation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="columns has-text-justified">
          <video width="100%" controls>
            <source src="static/videos/video_abs.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div>
  </div>
</section>



  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <!-- TODO: Replace with your paper abstract -->
            <p>
              Safe control techniques, such as Hamilton-Jacobi reachability, 
              provide principled methods for synthesizing safety-preserving 
              robot policies but typically assume hand-designed state spaces 
              and full observability. Recent work has relaxed these assumptions
              via <i>latent-space</i> safe control, where state representations
              and dynamics are learned jointly through world models that 
              reconstruct future high-dimensional observations (e.g., RGB images)
              from current observations and actions. This enables safety 
              constraints that are difficult to specify analytically 
              (e.g., spilling) to be framed as classification problems in 
              latent space, allowing controllers to operate directly from 
              raw observations. However, these methods assume that 
              safety-critical features are observable in the learned latent
              state. We ask: <i>when are latent state spaces sufficient
              for safe control?</i> To study this, we examine temperature-based
              failures, comparable to overheating in cooking or manufacturing
              tasks, and find that RGB-only observations can produce myopic
              safety behaviors, e.g., avoiding <i>seeing failure</i>
              states rather than preventing failure itself. To predict 
              such behaviors, we introduce a mutual information-based 
              measure that identifies when observations fail to capture 
              safety-relevant features. Finally, we propose a 
              multimodal-supervised training strategy that shapes the 
              latent state with additional sensory inputs during training,
              but requires no extra modalities at deployment, and validate
              our approach in simulation and on hardware with a Franka 
              Research 3 manipulator preventing a pot of wax from 
              overheating.          
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

<!-- Motivation / Problem Setup -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">When Latent Safety Filters Miss What Matters</h2>
        <div class="content has-text-justified">
          <figure class="image is-centered">
            <img src="static/images/frontfig_mm.png" alt="frontfig" width="2000">
            <p style="font-size: 0.9rem; margin-top: 0.5em;">
              We design a series of controlled experiments to test how latent safety filters behave under partially observable constraints. <i>Left</i>: We find that safety filters that rely on RGB inputs behave unreliably when they must enforce constraints, such as temperature limits, that are not easily observable. <i>Right</i>: Training with rich, safety-relevant multimodal supervision shapes the latent state representation to enable safe control (e.g., lifting the pan before overheating), even when the robot is deployed with only RGB inputs at runtime.
            </p>
          </figure>

          <p>
            Latent-space safety filters enable safe control directly from raw high-dimensional
            (e.g., RGB images) sensor inputs
            by training a world model which jointly learns compact state representations 
            and dynamics via observation-reconstruction. Safety constraints are specified by 
            training a classifier on the latent state. 
          </p>
          <figure class="image is-centered">
            <img src="static/images/wm_training.png" alt="World Models shape latent states by reconstructing observations">
          </figure>
          <p>Once trained, latent safety filters
            can be computed by solving a latent Hamilton-Jacobi fixed point equation which
            cooptimizes both a safety monitor and safety-preservering fallback controller.
          </p>
          <figure class="image is-centered">
            <img src="static/images/latent_hj.png" alt="Safety filters can be computing by solving a latent Hamilton-Jacobi fixed point equation">
          </figure>
          <p>
            However, prior works assumed that all safety-critical information is 
            observable from RGB sensor inputs. This may break down in many real-world tasks,
            such as cooking or welding, where safety often depends on hidden or indirectly observable 
            variables like temperature. When these unobserved factors drive unsafe outcomes, controllers 
            trained on RGB-only data can appear safe while actually failing to prevent danger.
          </p>
          <p>
            To study this challenge concretely, we focus on temperature as a representative partially observable safety variable.
            By examining heating tasks where RGB cameras cannot fully perceive temperature changes, 
            we show how latent safety filters behave when the true risk is partially observable,
            and how multimodal supervision only at training time can recover reliable safety behavior.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper Environments -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 center-title">Experiment Testbeds</h2>

        <div class="content has-text-justified">
          <p>
            We evaluate latent safety filters in both simulation and hardware environments designed to reveal how partial observability impacts safe behavior.
          </p>
          <figure class="image is-centered">
            <img src="static/images/mm_envs.png" alt="Simulation and hardware environments">
          </figure>

          <p>
            In simulation (<i>left</i>), we introduce the <strong>thermal unicycle</strong>, a 4-D unicycle model augmented with a latent heat variable that increases as the agent approaches a heat source. The agent receives either RGB or infrared (IR) images and must prevent overheating. This setup is intentionally simple and controllable, allowing us to isolate how safety filters behave when safety-relevant features are only partially observable. 
          </p>
          <p>
            On hardware (<i>right</i>), we use a <strong>Franka Research 3 manipulator</strong> heating a pot of wax. We collect both RGB and infrared (IR) observations during data collection, where the IR modality provides privileged information about temperature that is not observable from RGB images. 
            This setup allows us to study how the presence (or lack thereof) of safety critical information during training/deployment affects the downstream performance of latent safety filters. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper environments -->


<!-- [start] mutual information -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Mutual Information as a Measure of Observability</h2>
        <div class="content has-text-justified">
          <p>
          In latent state spaces, traditional notions of observability may not apply. Instead, 
          we use the notion of mutual information (MI) between high-D sensor observations and 
          safety outcomes as a way of measuring the observability of safety-relevant quantities.
          This metric quantifies the degree to which uncertainty over safety outcomes is reduced 
          by observing a particular input modality (e.g., RGB or infrared). We compute a 
          Barber-Agakov lower bound on MI between observations and binary safety labels to
          measure how well each modality captures safety-relevant features. Higher MI indicates
          that the modality more reliably encodes features necessary for safety prediction. 
          </p>

          <figure class="image is-centered">
            <img src="static/images/MI_metric.png" width="1100">
          </figure>
          <p>
            We report MI normalized by the empirical entropy of the safety labels 
            (normalized MI between 0 and 1). In both simulation, where RGB is insufficient
            for identifying failure by design, and hardware, we find that IR observations 
            exhibit much higher normalized MI than RGB alone, suggesting that RGB data 
            lacks sufficient safety information in these settings. Furthermore, we find 
            that the MI metrics are more indicative than traditional classification-based
            metrics, such as accuracy and balanced accuracy, when identifying degenerative
            latent states.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- [end] mutual information -->


<!-- latent state and imagination test -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Examining Latent Representation Quality</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate the quality of the learned latent representations by examining 
            how well they encode safety-relevant state information. Models trained only
            on RGB observations often produce latent states that fail to represent
            temperature, leading to visually correct but unsafe predictions. In contrast,
            our multimodal approach learns latent states that embed the underlying thermal 
            dynamics, enabling proactive interventions that maintain safety. 
          </p>
          <figure class="image is-centered">
            <img src="static/images/hw_qual_v3.png" alt="RGB-only training is unable to understand safety outcomes of actions" width="1100">
          </figure>
          <p>
            To quantify latent representation quality, we introduce two diagnostic tests.
            The <em>latent state test</em> measures how much safety-relevant information 
            (e.g., heat) is directly encoded in the learned latent state, while the 
            <em>latent dynamics test</em> evaluates whether the world model's open-loop 
            predictions understand how safety outcomes evolve over time. Together, these 
            tests reveal whether the learned latent space both contains and maintains 
            the safety features needed for effective safe control. 
          </p>
        </div>

        <div class="columns has-text-justified">
          <div class="column">
            <h3 class="title is-5">Latent State Test</h3>
            <figure class="image is-centered">
              <img src="static/images/latent_state_test.png" alt="RGB-only training is unable to understand safety outcomes of actions" width="1100">
            </figure>
          </div>

          <div class="column">
            <h3 class="title is-5">Latent Dynamics Test</h3>
            <figure class="image is-centered">
              <img src="static/images/latent_dynamics_test.png" alt="RGB-only training is unable to understand safety outcomes of actions" width="1100">
            </figure>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            The latent state and latent dynamics test results align with our previous MI-based 
            metric: the latent features degrade when safety-critical features are 
            not directly observable.  
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- simulation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Simulation Experiments</h2>

        <div class="content has-text-justified">
          <p>
            We first evaluate safety filter behavior in the <strong>thermal unicycle</strong> simulation. 
            The agent must navigate while avoiding overheating caused by proximity to a heat source. 
            We compare a safety filter trained with only RGB inputs to one trained with multimodal supervision using infrared (IR) data.
          </p>
        </div>

        <div class="columns has-text-justified">
          <div class="column">
            <h3 class="title is-4">RGB-Only Safety Filter</h3>
            <video width="100%" controls>
              <source src="static/videos/rgb_demo_dubins.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p>
              The agent avoids visually unsafe states but fails to prevent actual overheating.
            </p>
          </div>

          <div class="column">
            <h3 class="title is-4">Multimodal Safety Filter</h3>
            <video width="100%" controls>
              <source src="static/videos/mm_demo_dubins.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p>
              The agent predicts temperature rise early and moves out of the hot region before failure occurs.
            </p>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            Since temperature changes is only visible in the RGB input <i>after</i> leaving the red hot
            region, we notice myopic behavior the prevents seeing failure rather than preventing failure itself
            when deploying RGB-only latent safety filters. </p>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- hardware -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hardware Experiments</h2>

        <div class="content has-text-justified">
          <p>
            To validate our findings in the real world, we also deploy RGB-only and multimodal latent safety filters on a 
            <strong>Franka Research 3 manipulator</strong> heating a pot of wax. 
            The safety objective is to prevent overheating by lifting the pot before its temperature exceeds a threshold. 
          </p>
        </div>

        <div class="columns has-text-justified">
          <div class="column">
            <h3 class="title is-4">RGB-Only Safety Filter</h3>
            <video width="100%" controls>
              <source src="static/videos/rgb_demo.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p>
              The RGB-only safety filter fails to provide safe actions, causing the pot of wax to overheat.
            </p>
          </div>

          <div class="column">
            <h3 class="title is-4">Multimodal Safety Filter</h3>
            <video width="100%" controls>
              <source src="static/videos/mm_demo.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p>
              The multimodal safety filter anticipates overheating and lifts the pot of wax early, maintaining safety.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- hardware MM-RGB -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multimodal-Supervised RGB Safety Filter</h2>

        <div class="content has-text-justified">
          <p>
            While previous results confirm that directly observing safety relevant features 
            improves safe control, it may not be feasible to deploy robots with a full suite of
            sensors at scale.
          </p>
          <figure class="image is-centered">
            <img src="static/images/mm_supervised.png" alt="RGB-only training is unable to understand safety outcomes of actions" width="1100">
          </figure>
          <p>
            We propose a world model training strategy that shapes 
            the latent representation by reconstructing privileged IR data while only using RGB 
            data as an input modality. This world model training strategy forces the latent 
            representation to encode safety relevant quantities and learn to estimate 
            these quantities from RGB data at runtime.
          </p>
          <video width="100%" controls>
            <source src="static/videos/masked_demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p>
            The multimodal-supervised safety filter anticipates overheating and lifts 
            the pan before failure. Trained with RGB + IR data but deployed using only
            RGB, the controller maintains safety even under partial observability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?},
  author={Matthew Kim and Kensuke Nakamura and Andrea Bajcsy},
  journal={Conference/Journal Name},
  year={2025},
  url={https://cmu-intentlab.github.io/multisafe/}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
